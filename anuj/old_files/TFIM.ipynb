{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de084d1-3a75-46e8-a11b-1cc8511e480b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/east-1/vit-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\"\n",
    "\n",
    "import netket as nk\n",
    "import numpy as np\n",
    "import json\n",
    "from math import pi\n",
    "import jax\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from flax import linen as nn\n",
    "from typing import Any\n",
    "import optax\n",
    "import netket.experimental as nkx\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f27fbef-f1ce-44ba-8e19-0874f5faabbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 22\n",
    "hi = nk.hilbert.Spin(s=1 / 2, N=N, inverted_ordering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b927388-fb10-4399-b630-b9aed192fd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,\n",
       "         1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.],\n",
       "       [ 1.,  1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
       "         1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1.],\n",
       "       [ 1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
       "        -1., -1., -1., -1., -1., -1.,  1.,  1., -1.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi.random_state(jax.random.key(0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58cc3bc6-563c-4785-8e4d-12d4b5fdc9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netket.operator.spin import sigmax,sigmaz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce287218-8684-4bc8-99a7-afffcf2e434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma = -1\n",
    "H = sum([Gamma*sigmax(hi,i) for i in range(N)])\n",
    "g = 1\n",
    "V = Gamma*g\n",
    "H += sum([V*sigmaz(hi,i)*sigmaz(hi,(i+1)%N) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02fedf97-10fe-4b50-ad9b-7008a47c49ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4194304, 4194304)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "sparse_h = H.to_sparse()\n",
    "sparse_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccdf01bd-1ad5-4f33-9bce-b28998d7f77d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues with scipy sparse: [-28.03508409 -27.96365391]\n"
     ]
    }
   ],
   "source": [
    "eig_vals, eig_vecs = eigsh(sparse_h, k=2, which=\"SA\")\n",
    "\n",
    "print(\"eigenvalues with scipy sparse:\", eig_vals)\n",
    "\n",
    "E_gs = eig_vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592491d6-4791-4d6c-80bb-563961e66fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for g=1 at N=22 we get eigenvalues with scipy sparse: [-28.03508409 -27.96365391]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd278c9d-a4f5-4df6-83eb-69aafe782716",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = hi.random_state(size=64, key=jax.random.PRNGKey(0))\n",
    "input_batch = jnp.array(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e456badf-b314-4cd7-9b22-063a5cbdbfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_info(model):\n",
    "    \"\"\"Prints the model architecture and parameter count.\"\"\"\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    params = model.init(key, input_batch)\n",
    "    dtypes = jax.tree_util.tree_map(lambda x: x.dtype, params)\n",
    "    print(dtypes)\n",
    "    print(\"Model Architecture:\")\n",
    "    print(model)\n",
    "    total_params = sum(jnp.prod(jnp.array(param.shape)) for param in jax.tree_util.tree_leaves(params))\n",
    "    print(f\"Total Trainable Parameters: {total_params}\")\n",
    "\n",
    "def linear_solver_pinv_smooth(rcond=1e-6):\n",
    "    return lambda A, b: nk.optimizer.solver.pinv_smooth(A, b, rtol=rcond, rtol_smooth=rcond)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e599f4-55d6-4752-bf77-ec0fb2a650d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "rng, model_rng, sampler_rng = jax.random.split(rng, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc18218f-0672-4021-a6e1-cd5ea2b81b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTNQS(nn.Module):\n",
    "    Lx: int  # Lattice size in x direction (total sites for 1D input)\n",
    "    patch_size: int\n",
    "    d_model: int  # Embedding dimension\n",
    "    num_heads: int\n",
    "    num_layers: int\n",
    "    param_dtype: Any = jnp.complex64\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, σ):\n",
    "        # σ: (batch_size, N_sites)\n",
    "        # N_sites = Lx\n",
    "        batch_size = σ.shape[0]\n",
    "        N_sites = σ.shape[1]\n",
    "\n",
    "        # Extract patches: (batch_size, n_patches, patch_size)\n",
    "        patches = extract_patches_1d(σ, self.patch_size)\n",
    "\n",
    "        # Flatten patches: (batch_size, n_patches, patch_size)\n",
    "        # No reshaping required beyond patch extraction\n",
    "\n",
    "        # Linear embedding of patches into d-dimensional space\n",
    "        patch_embedding = nn.Dense(self.d_model, use_bias=True, param_dtype=self.param_dtype, name='patch_embedding')\n",
    "        x = patch_embedding(patches)  # (batch_size, n_patches, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        n_patches = x.shape[1]\n",
    "        pos_embedding = self.param('pos_embedding', nn.initializers.normal(stddev=0.05), (1, n_patches, self.d_model), self.param_dtype)\n",
    "        x = x + pos_embedding\n",
    "\n",
    "        # ViT encoder blocks\n",
    "        for _ in range(self.num_layers):\n",
    "            x = ViTEncoderBlock(d_model=self.d_model, num_heads=self.num_heads, param_dtype=self.param_dtype)(x)\n",
    "\n",
    "        # Hidden representation z\n",
    "        z = jnp.sum(x, axis=1)  # (batch_size, d_model)\n",
    "\n",
    "        # Final mapping to complex logarithm of amplitude\n",
    "        # Parameters w, b\n",
    "        w = self.param('w', nn.initializers.normal(stddev=0.1), (self.d_model,), jnp.complex64)\n",
    "        b = self.param('b', nn.initializers.normal(stddev=0.1), (self.d_model,), jnp.complex64)\n",
    "\n",
    "        # Compute pre-activation: (batch_size, d_model)\n",
    "        pre_activation = w * z + b\n",
    "\n",
    "        # Apply g(·) = logcosh(·)\n",
    "        g = lambda x: jnp.log(jnp.cosh(x))\n",
    "\n",
    "        # Handle complex inputs in logcosh\n",
    "        log_psi = jnp.sum(g(pre_activation), axis=-1)  # (batch_size,)\n",
    "        return log_psi\n",
    "\n",
    "\n",
    "def extract_patches_1d(σ, patch_size):\n",
    "    \"\"\"\n",
    "    Extract patches for 1D input without reshaping.\n",
    "    Args:\n",
    "        σ: (batch_size, Lx), where Lx is the lattice size.\n",
    "        patch_size: Size of each patch.\n",
    "    Returns:\n",
    "        patches: (batch_size, n_patches, patch_size), where n_patches = Lx // patch_size.\n",
    "    \"\"\"\n",
    "    batch_size, Lx = σ.shape\n",
    "    n_patches = Lx // patch_size\n",
    "\n",
    "    # Truncate σ to ensure full patches\n",
    "    σ = σ[:, :n_patches * patch_size]\n",
    "\n",
    "    # Split into patches along the last axis\n",
    "    patches = σ.reshape(batch_size, n_patches, patch_size)\n",
    "    return patches\n",
    "\n",
    "class ViTEncoderBlock(nn.Module):\n",
    "    d_model: int\n",
    "    num_heads: int\n",
    "    param_dtype: Any = jnp.complex64\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Pre-Layer Normalization\n",
    "        y = nn.LayerNorm(dtype=self.param_dtype, param_dtype=self.param_dtype)(x)\n",
    "        # Factored Multi-Head Attention\n",
    "        y = FactoredMultiHeadAttention(num_heads=self.num_heads, d_model=self.d_model, param_dtype=self.param_dtype)(y)\n",
    "        # Skip connection\n",
    "        x = x + y\n",
    "\n",
    "        # Pre-Layer Normalization\n",
    "        y = nn.LayerNorm(dtype=self.param_dtype, param_dtype=self.param_dtype)(x)\n",
    "        # Feedforward network\n",
    "        y = nn.Dense(4*self.d_model, dtype=self.param_dtype, param_dtype=self.param_dtype)(y)\n",
    "        #y = nn.Dense(self.d_model, dtype=self.param_dtype, param_dtype=self.param_dtype)(y)\n",
    "        y = nn.gelu(y)\n",
    "        y = nn.Dense(self.d_model, dtype=self.param_dtype, param_dtype=self.param_dtype)(y)\n",
    "        # Skip connection\n",
    "        x = x + y\n",
    "\n",
    "        return x\n",
    "\n",
    "class FactoredMultiHeadAttention(nn.Module):\n",
    "    num_heads: int\n",
    "    d_model: int\n",
    "    param_dtype: Any = jnp.complex64\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        batch_size, n_patches, _ = x.shape\n",
    "        head_dim = self.d_model // self.num_heads\n",
    "\n",
    "        # Linear projection for V: Values\n",
    "        V = nn.DenseGeneral((self.num_heads, head_dim), axis=-1, dtype=self.param_dtype, param_dtype=self.param_dtype, use_bias=False, name='V_proj')(x)\n",
    "        V = V.transpose(0, 2, 1, 3)  # (batch_size, num_heads, n_patches, head_dim)\n",
    "\n",
    "        # Compute attention weights αF_ij = p_{i-j}\n",
    "        positions = jnp.arange(n_patches)\n",
    "        relative_positions = positions[None, :] - positions[:, None]  # (n_patches, n_patches)\n",
    "\n",
    "        # Positional biases p: (num_heads, 2 * n_patches - 1)\n",
    "        p = self.param('p', nn.initializers.normal(stddev=0.02), (self.num_heads, 2 * n_patches - 1), self.param_dtype)\n",
    "\n",
    "        # Map relative positions to indices\n",
    "        relative_position_indices = relative_positions + n_patches - 1  # Shift indices to be >=0\n",
    "\n",
    "        # Get attention weights: (num_heads, n_patches, n_patches)\n",
    "        attention_weights = p[:, relative_position_indices]\n",
    "\n",
    "        # Expand to include batch dimension: (batch_size, num_heads, n_patches, n_patches)\n",
    "        attention_weights = jnp.broadcast_to(attention_weights[None, :, :, :], (batch_size, self.num_heads, n_patches, n_patches))\n",
    "\n",
    "        # # Softmax over the key dimension\n",
    "        # attention_weights = nn.softmax(attention_weights, axis=-1)\n",
    "\n",
    "        # Compute attention output\n",
    "        attention_output = jnp.matmul(attention_weights, V)  # (batch_size, num_heads, n_patches, head_dim)\n",
    "\n",
    "        # Reshape and project back to d_model\n",
    "        attention_output = attention_output.transpose(0, 2, 1, 3).reshape(batch_size, n_patches, self.d_model)\n",
    "        output = nn.Dense(self.d_model, dtype=self.param_dtype, param_dtype=self.param_dtype, use_bias=False, name='output_proj')(attention_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad7c89-5322-4e2a-9f7a-098d8284f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "machine = ViTNQS(\n",
    "    Lx=N,\n",
    "    patch_size=4,\n",
    "    d_model=32,\n",
    "    num_heads=2,\n",
    "    num_layers=1,\n",
    "    param_dtype=jnp.complex64\n",
    ")\n",
    "\n",
    "print_model_info(machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0105acf-dcf3-4897-ad6b-4398eeeae1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [0.1]\n",
    "rconds = [1e-6]\n",
    "diag_shifts = [1e-6]\n",
    "\n",
    "n_chains = 32\n",
    "n_samples = 4096\n",
    "n_discard_per_chain = 0\n",
    "chunk_size = 4096\n",
    "\n",
    "iterations = 200\n",
    "timeout = 1200\n",
    "max_norm = 1.0\n",
    "clip = optax.clip_by_global_norm(max_norm=max_norm)\n",
    "\n",
    "for lr in lrs:\n",
    "    for rcond in rconds:\n",
    "        for diag_shift in diag_shifts:\n",
    "            print(f\"Running for lr={lr} and rcond={rcond}, diag_shift={diag_shift}\")\n",
    "\n",
    "            sampler = nk.sampler.MetropolisLocal(\n",
    "                hi,\n",
    "                n_chains=n_chains,\n",
    "                dtype=jnp.int8\n",
    "            )\n",
    "\n",
    "            # Initialize variational state with fixed seed\n",
    "            vstate_rng, rng = jax.random.split(rng)\n",
    "            vstate = nk.vqs.MCState(\n",
    "                sampler=sampler,\n",
    "                model=machine,\n",
    "                n_samples=n_samples,\n",
    "                n_discard_per_chain=n_discard_per_chain,\n",
    "                chunk_size=chunk_size,\n",
    "                seed=vstate_rng  # Set variational state seed\n",
    "            )\n",
    "            \n",
    "            # Define optimizer\n",
    "            lr_schedule = optax.cosine_decay_schedule(init_value=lr, decay_steps=iterations)\n",
    "            optimizer = optax.chain(\n",
    "                optax.zero_nans(),\n",
    "                clip,\n",
    "                nk.optimizer.Sgd(learning_rate=lr_schedule)\n",
    "            )\n",
    "            \n",
    "            gs = nkx.driver.VMC_SRt(\n",
    "                H,\n",
    "                optimizer,\n",
    "                diag_shift=diag_shift,\n",
    "                variational_state=vstate,\n",
    "                jacobian_mode=\"complex\",\n",
    "                linear_solver_fn=linear_solver_pinv_smooth(rcond=rcond)\n",
    "            )\n",
    "            \n",
    "            # Run optimization\n",
    "            gs.run(\n",
    "                n_iter=iterations,\n",
    "                out=f\"state_lr_{lr}_rcond_{rcond}_shift_{diag_shift}\",\n",
    "                callback=[\n",
    "                    nk.callbacks.Timeout(timeout=timeout),\n",
    "                    nk.callbacks.InvalidLossStopping(monitor=\"mean\", patience=1)\n",
    "                ]\n",
    "            )\n",
    "    \n",
    "            # Save results\n",
    "            data = json.load(open(f\"state_lr_{lr}_rcond_{rcond}_shift_{diag_shift}.log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d620fd-415c-47e9-81ba-4e4902f2fa18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit-env",
   "language": "python",
   "name": "vit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
